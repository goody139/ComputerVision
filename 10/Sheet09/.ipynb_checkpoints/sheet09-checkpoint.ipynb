{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "107717b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e69b169290d2439dd3e5b4005f092407",
     "grade": false,
     "grade_id": "cell-0d1b4545a5ac35b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Computer Vision (Winter Term 2021/22) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Axel Schaffland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf6702",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0dd873d54367b385321eb2a6f4463a74",
     "grade": false,
     "grade_id": "cell-f3a089ba6ba000ad",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 09: Local Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b141e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5eb44d859bb8baf48db7b2321042659c",
     "grade": false,
     "grade_id": "cell-bba575acc955c12f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Tuesday, January 25, 2022**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0a8b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12b81540024f0bf98fb934e8dd6b65ac",
     "grade": false,
     "grade_id": "cell-968c945b0dc138ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1: Local features and interest points [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c657edc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82eadf947382a48bb4ec9d672aa59894",
     "grade": false,
     "grade_id": "cell-54385603796cbdc5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain in your own words: What are *local features*? How are they used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda5848",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a78828db3a9676a17422598fc4795e8",
     "grade": true,
     "grade_id": "cell-9ab13a0e8484d6b9",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Basically there are two types of features being extracted from the images based on the application, local and global features. Features are sometimes referred to as descriptors. Global descriptors are generally used in image retrieval, object detection and classification, while the local descriptors are used for object recognition or identification.\n",
    "There is a large difference between detection and identification. Detection is finding the existence of something/object (Finding whether an object is exist in image/video) where as Recognition is finding the identity (Recognizing a person/object) of an object.\n",
    "Local features can be considered as certain patterns, or specific structures in images. Those local features are meant to be different from its surroundings with respect to texture, color and intensity. To name a few examples,Blobs and edge pixels are local features.\n",
    "\n",
    "They are used as ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed740c6f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "621e3785e27c2a56e40841056f9617a7",
     "grade": false,
     "grade_id": "cell-9a58a036bbdcba63",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "solution": "hidden"
   },
   "source": [
    "**b)** What are *interest points* and what are they used for? What properties are desirable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48eb8e2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a278d9e80fb283e15ecba3515c276298",
     "grade": true,
     "grade_id": "cell-b964929c6f2d5419",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": "hidden"
   },
   "source": [
    "Interest points, also called salient points, are points in an image which have a well-defined position and can be robustly detected. Usually, it is also exceptional from its neighborhood. Those points are used for...\n",
    "\n",
    "An IP is just a point and as a such not very relevant. For image\n",
    "processing, an IP must be associated with a region. The size of the\n",
    "associated region depends on the detector type:For recognition of entities, the region associated with an IP must\n",
    "be described by suitable set of features\n",
    "Those region descriptors can be used as a basis for Object recognition.\n",
    "\n",
    "\n",
    "We want an interesting point to be rich concerning image content withing the local window which is referring to brightness variation, color variation and so on. In other words, it should be \"interesting\", or \"special\" enough so that we would consider to match such a point. Some kind of robustness might be useful too, like being invariance to image rotation, scaling and lightning. This makes it possible to find a certain point under arbitrary conditions. Moreover, it is advantageous to choose an interest point with a well defined position, because if you want to recognize a certain point in an image it is useful to have appropriate information about where this point is. In a nutshell, you may summarize the most important properties as Saliency and Stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c448d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "897486156a3ead89579bf8d859fabc8d",
     "grade": false,
     "grade_id": "cell-bffcd57e4ade7360x",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2: Computing interest points [6 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb4d96",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "080280cd532a1a4c48ad75595af6b302",
     "grade": false,
     "grade_id": "cell-moravec-q",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain in your own words the idea of the Moravec IP operator. What are its properties? Implement this IP operator and apply it to the image `lighthouse.png`. Try different window sizes and threshold values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4eee3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f71a7dbedc335281ea82e554074fcdd",
     "grade": true,
     "grade_id": "cell-e5a892a0e5b57489",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The Moravec IP operator is a basic IP detector. It is not currently used anymore, since it is not working out properly, however advanced operator are built upon this basic idea. First of all, the operator is based on one claim, namely that one window is salienf if it is \"unique\" concerning its surroundings. Unique in a way, that it differs from its surroundings. Further, one simple idea is to shift a certain window by one pixel and compare it to itself. The operator is now comparing the shifted window in 4 directions. \n",
    "\n",
    "Certain Properties are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58c7161e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58fe5626356710b9276dfb9c25bc2025",
     "grade": true,
     "grade_id": "cell-moravec-a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(472, 307)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15620/3114159909.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmoravec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[0mthresholded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15620/3114159909.py\u001b[0m in \u001b[0;36mmoravec\u001b[1;34m(img, window_size)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmin_v\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[1;31m#corners.append((x, y))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[1;31m# wo der min diff ist die coordinates nehmen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m            \u001b[1;31m# corners.append((x, y))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import imageio\n",
    "import numpy as np\n",
    "import scipy.ndimage as nd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "img = Image.open('images/images/lighthouse.png').convert(\"L\")\n",
    "\n",
    "window_size = 3\n",
    "threshold = .2\n",
    "\n",
    "def moravec(img, window_size):\n",
    "    \"\"\"Moravec corner detector.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    img: np.ndarray\n",
    "        The input image\n",
    "    window_size: int\n",
    "        The size of the window to consider\n",
    "\n",
    "    Results\n",
    "    -------\n",
    "    response: np.ndarray\n",
    "        Response (of the same size as img), indicating interest points.\n",
    "    \"\"\"\n",
    "    response = np.zeros_like(img)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    print(img.size)\n",
    "    \n",
    "    # squared window_size\n",
    "    l = window_size \n",
    "    k = window_size\n",
    "     \n",
    "    corners = []\n",
    "    xy_shifts = [(1, 0), (1, 1), (0, 1), (-1, 1)]\n",
    "\n",
    "    \n",
    "    # loop through every pixel \n",
    "    for y in range(1, img.size[1]-1):\n",
    "        for x in range(1, img.size[0]-1):\n",
    "            \n",
    "            min_v = []\n",
    "            # loop through 4 shifts \n",
    "            for shift in xy_shifts:\n",
    "                diff = img.getpixel((x + shift[0], y + shift[1]))\n",
    "                diff = diff - img.getpixel((x, y))\n",
    "                diff = np.power(diff, 2)\n",
    "                min_v.append(diff)\n",
    "            \n",
    "            \n",
    "            min_v = min(min_v)\n",
    "            # corner if diff exceeds threshold\n",
    "            if min_v > threshold:\n",
    "                #corners.append((x, y))\n",
    "                response[x,y] = (x, y)\n",
    "            # wo der min diff ist die coordinates nehmen \n",
    "           # corners.append((x, y))\n",
    "    \n",
    "    print(response.shape)\n",
    "    #return corners\n",
    "\n",
    "    return response\n",
    "\n",
    "response = moravec(img, window_size)\n",
    "\n",
    "thresholded = np.zeros_like(response)\n",
    "thresholded[response > threshold] = 1\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(2,2,1); plt.imshow(img, cmap='gray'); plt.title('Original')\n",
    "plt.subplot(2,2,2); plt.imshow(response, cmap='viridis'); plt.title('Moravec \"heatmap\"')\n",
    "plt.colorbar()\n",
    "plt.subplot(2,2,3); plt.imshow(thresholded, cmap='gray'); plt.title(f'Thresholded (>{threshold})')\n",
    "plt.subplot(2,2,4); plt.imshow(img[1:-1,:-1], cmap='gray'); plt.title('Corners')\n",
    "mask = np.zeros(thresholded.shape + (4,), dtype=int)\n",
    "mask[:,:,0] = 255\n",
    "mask[:,:,3] = thresholded*255\n",
    "plt.imshow(mask, interpolation='none')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a07f416",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c59a8fc69fc6f8979b64dd6de21f197",
     "grade": false,
     "grade_id": "cell-harris-q",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** How does the Harris corner detector work and in what sense does it improve the Moravec IP operator. Implement the Harris corner detector and apply it to `lighthouse.png`. Again, try different \"window sizes\" (values for $\\sigma$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec665f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "caeee663722a27d0636dde08ede71424",
     "grade": true,
     "grade_id": "cell-d51a9747dd39f80b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe077cd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba233bc1af8194eb9332026cbe586e0f",
     "grade": true,
     "grade_id": "cell-harris-a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as nd\n",
    "\n",
    "k = .04\n",
    "window_size = 3\n",
    "\n",
    "img = imageio.imread('images/lighthouse.png', pilmode='F')/255.\n",
    "\n",
    "def harris(img, window_size=3, k=0.04):\n",
    "    \"\"\"Harris corner detector.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    img: np.ndarray\n",
    "        The input image\n",
    "    window_size: int\n",
    "        The size of the window to consider\n",
    "    k: float\n",
    "        The parameter k\n",
    "        \n",
    "    Results\n",
    "    -------\n",
    "    response: np.ndarray\n",
    "        Response (of the same size as img), indicating interest points.\n",
    "    \"\"\"\n",
    "    response = np.zeros_like(img)\n",
    "    # YOUR CODE HERE\n",
    "    return response\n",
    "\n",
    "response = harris(img, window_size, k)\n",
    "\n",
    "thresholded = np.zeros_like(response)\n",
    "thresholded[response > threshold] = 1\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(2,2,1); plt.imshow(img, cmap='gray'); plt.title('Original')\n",
    "plt.subplot(2,2,2); plt.imshow(response, cmap='viridis'); plt.title('Harris \"heatmap\"')\n",
    "plt.colorbar()\n",
    "plt.subplot(2,2,3); plt.imshow(thresholded, cmap='gray'); plt.title(f'Thresholded (>{threshold})')\n",
    "plt.subplot(2,2,4); plt.imshow(img[1:-1,:-1], cmap='gray'); plt.title('Corners')\n",
    "mask = np.zeros(thresholded.shape + (4,), dtype=int)\n",
    "mask[:,:,0] = 255\n",
    "mask[:,:,3] = thresholded*255\n",
    "plt.imshow(mask, interpolation='none')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5719f6a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef983abb2daa1b75549bb27104a8cea7",
     "grade": false,
     "grade_id": "cell-sift-concepts",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Understanding SIFT [4 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a126a72",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a02aad2b39f4233e81c03fbe4c62458e",
     "grade": false,
     "grade_id": "cell-header03",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "*Hint:* For this and the next exercise it is really helpful to take a look at:\n",
    "* [[Lowe04]](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf) David G. Lowe: Distinctive Image Features from \n",
    "Scale-Invariant Keypoints. IJCV, 2004."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab66fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bac39a0c7bccc405899878153642b3a",
     "grade": false,
     "grade_id": "cell-sift-concepts-q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain the two main steps that are typically performed to construct local features. How are these steps realized in SIFT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4627dfb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "895e45fbb974fab050444e35f7179f49",
     "grade": true,
     "grade_id": "cell-sift-concepts-a1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89a0f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7beab6180f84a8bba8a94abe566f8d4",
     "grade": false,
     "grade_id": "cell-sift-concepts-q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** SIFT key points (interest points) are claimed to invariant to changes in (a) scale, (b) orientation, (c) illumination, and (d) affine transformations. Describe how SIFT key points are computed and explain how this construction contributes to each of these individual goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9bfac9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13fa84fd693058ab3b9dcbfd322adf61",
     "grade": true,
     "grade_id": "cell-sift-concepts-a2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307c22a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5aac23cd69fccb7e31797e70c73cdef0",
     "grade": false,
     "grade_id": "cell-sift-concepts-q3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** Explain how SIFT descriptors are constructed and motivate this construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e3a8c6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8e1edec66346f5587bf471a58fceb0a",
     "grade": true,
     "grade_id": "cell-sift-concepts-a3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19317a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdaa8f9336edd0ce6aad5724079a6e58",
     "grade": false,
     "grade_id": "cell-sift-concepts-q4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**d)** *Object recognition:* Design an object recognition system based on local features (like SIFT)? Describe how to train such a system and how to perform the recognition task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4113e6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fe367e86a7a3ddfc0390354db45ae92",
     "grade": true,
     "grade_id": "cell-sift-concepts-a4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854202ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "735cf80d84425694affdbf7101a469a8",
     "grade": false,
     "grade_id": "cell-sift-apply",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 4: Applying SIFT [5 Points]\n",
    "\n",
    "In this exercise we invite you to explore the SIFT implementation provided by OpenCV. You may use whatever functions you find in that library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d2923",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2449f420387d9a2cba34937b54120273",
     "grade": false,
     "grade_id": "cell-sift-apply-q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** *Keypoint detection*: look up how to detect SIFT keypoints with OpenCV and find keypoints in the image `lighthouse.png`. Show the keypoints in the image. Find ways to indicate scale and direction of the keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b72f18",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54cf184af272d1ee10a4178d1de0eb16",
     "grade": true,
     "grade_id": "cell-5851a7631bd0e64f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread('images/lighthouse.png')\n",
    "assert img is not None, \"Error loading image\"\n",
    "\n",
    "img_keypoints = np.zeros_like(img)\n",
    "img_keypoints2 = np.zeros_like(img)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "plt.figure(figsize=(20, 30))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow(img)\n",
    "plt.title('The original image')\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(img_keypoints)\n",
    "plt.title('Keypoints detected by sift')\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(img_keypoints2)\n",
    "plt.title('Keypoints detected by sift')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd77eb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d027184a729fc0ebcdcedaa9e19eaa9",
     "grade": false,
     "grade_id": "cell-sift-opencv-acquire",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** *Feature matching*: Take two images of the same object (using your webcam, or if this does not work by some other technique), compute their SIFT features and match them. Show which keypoints are matched. Try how different position, scale, and orientation influence the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: capture two images with the webcam (and store them on disk)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "name1 = input(\"Enter name for the first image ...\") or \"image1\"\n",
    "camera = cv2.VideoCapture(0)\n",
    "time.sleep(.5)\n",
    "ok, frame1 = camera.read()\n",
    "camera.release()\n",
    "assert frame1 is not None, \"Camera did not work!\"\n",
    "cv2.imwrite(name1 + '.png', frame1)\n",
    "\n",
    "name2 = input(\"Enter name for the second image ...\") or \"image2\"\n",
    "time.sleep(.5)\n",
    "camera = cv2.VideoCapture(0)\n",
    "time.sleep(.5)\n",
    "ok, frame2 = camera.read()\n",
    "camera.release()\n",
    "assert frame2 is not None, \"Camera did not work!\"\n",
    "cv2.imwrite(name2 + '.png', frame2)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.suptitle(\"Frame: {}, type={}\".format(frame1.shape, frame1.dtype))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(frame1)\n",
    "plt.title(name1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(frame2)\n",
    "plt.title(name2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fc1d9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d72f10d548b8ce8b04aa419857eaebe7",
     "grade": true,
     "grade_id": "cell-259d28f7246aa801",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Part 2: load a pair of images, compute their SIFT features and match them. \n",
    "\n",
    "img1 = cv2.imread('image1.png')\n",
    "img2 = cv2.imread('image2.png')\n",
    "\n",
    "assert img1 is not None, \"Error loading image 1\"\n",
    "assert img2 is not None, \"Error loading image 2\"\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c5bcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f92806dbd8e94e6942a2b90521f62af0",
     "grade": false,
     "grade_id": "cell-d6cfb3a3bfc2ea36",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** *Object recognition*: build a small database of objects by taking snapshots and storing them along with their SIFT descriptors. Then try to query your database by taking new images and retrieving the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4f37d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf1c5084a9ab4add173fb3d06407dcef",
     "grade": false,
     "grade_id": "cell-7775482fe675632a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Part 1: store objects in the database\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "name = input(\"Enter name for new object ...\") or \"new object\"\n",
    "camera = cv2.VideoCapture(0)\n",
    "time.sleep(0.5)\n",
    "ok, frame = camera.read()\n",
    "camera.release()\n",
    "assert frame is not None, \"Camera does not work!\"\n",
    "\n",
    "# Compute features and store them in your database\n",
    "\n",
    "\n",
    "if not 'sift' in globals():\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "keypoints, descriptor = sift.detectAndCompute(frame, None)\n",
    "\n",
    "if not 'database' in globals() or not isinstance(database, dict):\n",
    "    database = {name: descriptor}\n",
    "else:\n",
    "    database[name] = descriptor\n",
    "\n",
    "print(database.keys())\n",
    "import imageio\n",
    "imageio.imwrite(name + '.png', frame[:,:,::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(database['r2d2']))\n",
    "print(len(database['abc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a747e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f35622553c5d4fe44532d3b1cf24f86b",
     "grade": false,
     "grade_id": "cell-7ae9427fc90943c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Part 2: retrieve objects from the database\n",
    "\n",
    "#input(\"Press enter to capture an image ...\")\n",
    "camera = cv2.VideoCapture(0)\n",
    "time.sleep(0.5)\n",
    "ok, frame = camera.read()\n",
    "camera.release()\n",
    "assert frame is not None, \"Camera did not work!\"\n",
    "\n",
    "# Now look up the best matching object from your database\n",
    "\n",
    "\n",
    "assert 'database' in globals() or not isinstance(database, dict), \"No database available\"\n",
    "\n",
    "# get the keypoints\n",
    "if not 'sift' in globals():\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "keypoints, descriptor = sift.detectAndCompute(frame, None)\n",
    "\n",
    "# prepare the matcher\n",
    "FLANN_INDEX_KDITREE=0\n",
    "flannParam = dict(algorithm=FLANN_INDEX_KDITREE, tree=5)\n",
    "flann = cv2.FlannBasedMatcher(flannParam, {})\n",
    "# store images from our database\n",
    "flann.add(list(database.values()))\n",
    "for d in database.values():\n",
    "    flann.add(d)\n",
    "\n",
    "# count matches\n",
    "counter = np.zeros(len(database), dtype=np.int)\n",
    "matches = flann.knnMatch(descriptor, k=2)\n",
    "for m,n in matches:\n",
    "    if (m.imgIdx == n.imgIdx) or (m.distance < 0.75 *n.distance):\n",
    "        counter[m.imgIdx] += 1\n",
    "\n",
    "# output the results\n",
    "print(\"Matches:\")\n",
    "for n, c in zip(database, counter):\n",
    "    print(f\"  image '{n}': {c}\")\n",
    "\n",
    "print(f\"Best match: '{list(database)[counter.argmax()]}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33572543",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b31cbd2f4a2f5a764788d5d00a626c02",
     "grade": false,
     "grade_id": "cell-6395853a8e864885",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 5: Rectangle features [3 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae95020",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3fb0b07e7f62c8a4398e9d59f8c4446",
     "grade": false,
     "grade_id": "cell-3609ac11e5e0672e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain the idea of rectangle features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11bfc4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fbd956f9c1a61628dec52b261de15a0",
     "grade": true,
     "grade_id": "cell-f37d38016fed4485",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Compared to SIFT, rectangle feature are not related to interesting points. Rather, so called sliding windows are used. They are moved along an image and perform detection via recognition which means nothing other than sliding windows being classified. Therefore, both steps are done at the same time. In Using so called integral images, this procedure can be speeded up, this is necessary due to the amount of required sliding windows. Simple rectangle features are built of a black and a white rectangular part. The feature value of the rectangle features are defined by the difference of the integral over the white and the black area. Eventually, the object recognition is done by a trained cascade of classifier which are evaluating simple rectangle features in the search windows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d76b61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a724a7645da60d3ab18f70af3e5fd3b7",
     "grade": false,
     "grade_id": "cell-d3c3d41d87c88354",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What is an integral image and how does it help to compute rectangle features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec42467",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a60945cd27bdb2982e907742ca12e8da",
     "grade": true,
     "grade_id": "cell-a2efbb9399590776",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "An integral image is simply the sum of each pixel concerning the original image that is above or to the left. Generally, these images are used in order to make the computation of the rectangle features easier. The computation gets easier because the integral image already gives information about the rectangle feature in a way that only 3 additions are neccessary when it comes to the computational cost. What makes the integral image interesting is also the feature of being independent of the rectangle's size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2fee7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "497416e246c85a655037654d7cd00f41",
     "grade": false,
     "grade_id": "cell-5e49e7277ee7e953",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** What is a classifier cascade and how is it used in the context of rectangle features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b426a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93a49f5ff32477b6e9634c130e9ef305",
     "grade": true,
     "grade_id": "cell-b7040762cd8eb47c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a069d7ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0499644b8dbc8a138f81d07448e0aaed",
     "grade": false,
     "grade_id": "cell-41e9db05dd734b7e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**d)** Compare SIFT and rectangle features. Name pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af47246f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8218a57297e4ad5e149ee94d13653ba8",
     "grade": true,
     "grade_id": "cell-ecd8cd2f552ee351",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
